{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable descriptions of original data set\n",
    "|Item|Name|Description|\n",
    "|:--:|:--|:--|\n",
    "|1|\tYear\t|1987-2008|\n",
    "|2|\tMonth\t|1-12|\n",
    "|3|\tDayofMonth\t|1-31|\n",
    "|4|\tDayOfWeek\t|1 (Monday) - 7 (Sunday)|\n",
    "|5|\tDepTime\t|actual departure time (local, hhmm)|\n",
    "|6|\tCRSDepTime\t|scheduled departure time (local, hhmm)|\n",
    "|7|\tArrTime\tactual |arrival time (local, hhmm)|\n",
    "|8|\tCRSArrTime\t|scheduled arrival time (local, hhmm)|\n",
    "|9|\tUniqueCarrier\t|unique carrier code|\n",
    "|10|\tFlightNum\t|flight number|\n",
    "|11|\tTailNum\tplane |tail number|\n",
    "|12|\tActualElapsedTime\t|in minutes|\n",
    "|13|\tCRSElapsedTime\t|in minutes|\n",
    "|14|\tAirTime\t|in minutes|\n",
    "|15|\tArrDelay\t|arrival delay, in minutes|\n",
    "|16|\tDepDelay\t|departure delay, in minutes|\n",
    "|17|\tOrigin\t|origin IATA airport code|\n",
    "|18|\tDest\t|destination IATA airport code|\n",
    "|19|\tDistance\t|in miles|\n",
    "|20|\tTaxiIn\t|taxi in time, in minutes|\n",
    "|21|\tTaxiOut\t|taxi out time in minutes|\n",
    "|22|\tCancelled\t|was the flight cancelled?|\n",
    "|23|\tCancellationCode\t|reason for cancellation (A = carrier, B = weather, C = NAS, D = security)|\n",
    "|24|\tDiverted\t|1 = yes, 0 = no|\n",
    "|25|\tCarrierDelay\t|in minutes|\n",
    "|26|\tWeatherDelay\t|in minutes|\n",
    "|27|\tNASDelay\t|in minutes|\n",
    "|28|\tSecurityDelay\t|in minutes|\n",
    "|29|\tLateAircraftDelay\t|in minutes|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mattbaldree/Google Drive/smu/quantifying/QTW_14/QTW_14\r\n"
     ]
    }
   ],
   "source": [
    "# general stuff\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('compute.use_bottleneck', True)\n",
    "pd.set_option('compute.use_numexpr', True)\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask import compute, persist\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flags\n",
    "\n",
    "PURGE_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:55360\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787' target='_blank'>http://127.0.0.1:8787</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>10.31 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:55360' processes=4 cores=4>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start Dask distributed client and print out stats\n",
    "\n",
    "c = Client()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if loading data, then purge existing data directory.\n",
    "\n",
    "if PURGE_DATA:\n",
    "    # delete /data directory\n",
    "    from shutil import rmtree\n",
    "\n",
    "    path = 'data'\n",
    "    if os.path.exists(path):\n",
    "        rmtree(path)\n",
    "    \n",
    "    # make /data if it doesn't exist\n",
    "    path = 'data'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if loading data, download files and decompress them in parallel.\n",
    "\n",
    "DOWNLOAD_ONE_FILE_ONLY = False\n",
    "\n",
    "if PURGE_DATA:\n",
    "    import urllib.request\n",
    "    import shutil\n",
    "    import bz2\n",
    "    \n",
    "    def download_file(baseurl, yr):\n",
    "        file_name = ''\n",
    "\n",
    "        url_of_data_file = baseurl%(yr)\n",
    "        file_name = 'data/%d.csv'%(yr)\n",
    "        size = 0\n",
    "\n",
    "        print('downloading', url_of_data_file, 'to', file_name)\n",
    "        decompressor = bz2.BZ2Decompressor()\n",
    "\n",
    "        # download file and decompress it\n",
    "        with urllib.request.urlopen(url_of_data_file) as response, open(file_name, 'wb') as out_file:\n",
    "                data = decompressor.decompress(response.read())\n",
    "                out_file.write(data)\n",
    "                size = len(data)\n",
    "                print('file size (MB)', locale.format('%.1f', size/1000000, grouping=True))\n",
    "\n",
    "        return(file_name, size)\n",
    "    \n",
    "    def print_files(files):\n",
    "        totalSize = 0        \n",
    "        for f in files:\n",
    "            size = f[1]/1000000\n",
    "            totalSize += size\n",
    "            print('downloaded file:', f[0], ', of size (MB):', \n",
    "                  locale.format('%.1f', size, grouping=True))\n",
    "            \n",
    "        print('Number of files downloaded:', len(files), 'for a total size (MB):', \n",
    "              locale.format('%.1f', totalSize, grouping=True))\n",
    "\n",
    "\n",
    "    if DOWNLOAD_ONE_FILE_ONLY:\n",
    "        # testing\n",
    "        download_file('http://stat-computing.org/dataexpo/2009/%d.csv.bz2', 1988)\n",
    "    else:    \n",
    "        # download airline data from 1987 to 2009\n",
    "        yrs = range(1987, 2009)\n",
    "        baseurl = 'http://stat-computing.org/dataexpo/2009/%d.csv.bz2'\n",
    "\n",
    "        from dask import delayed\n",
    "        download_file = delayed(download_file)\n",
    "\n",
    "        files = [download_file(baseurl, yr) for yr in yrs]\n",
    "        files = delayed(files)\n",
    "\n",
    "        %time files = files.compute()   \n",
    "      \n",
    "        print_files(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Click on Dashboard URL printed out above to see the distributed work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the head of a csv file\n",
    "\n",
    "print('csv file format')\n",
    "!head data/1987.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv files into a dataframe in parallel\n",
    "\n",
    "LOAD_ONE_FILE_ONLY = False\n",
    "\n",
    "if LOAD_ONE_FILE_ONLY:\n",
    "    filename = os.path.join('data', '2000.csv')\n",
    "else:\n",
    "    filename = os.path.join('data', '*.csv')\n",
    "print('Loading', filename, 'files')\n",
    "\n",
    "import dask.dataframe as dd\n",
    "%time df_csv = dd.read_csv(filename, assume_missing=True, \\\n",
    "                           dtype={'TailNum':np.object, 'CancellationCode':np.object}, \\\n",
    "                           storage_options={'anon': True}).rename(columns=str.lower)\n",
    "\n",
    "print(df_csv.dtypes)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop columns we don't need or want\n",
    "df_csv = df_csv.drop(['tailnum', 'actualelapsedtime', 'crselapsedtime', 'airtime', \\\n",
    "             'taxiin', 'taxiout', 'cancelled', 'cancellationcode', \\\n",
    "             'diverted', 'carrierdelay', 'weatherdelay', 'nasdelay', \\\n",
    "             'securitydelay', 'lateaircraftdelay'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataframe\n",
    "\n",
    "number_of_items = len(df_csv)\n",
    "locale.format('%d', number_of_items, grouping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 origin airports\n",
    "\n",
    "origin_counts = df_csv.origin.value_counts().head(10)\n",
    "print('Top origin airports')\n",
    "print(origin_counts)\n",
    "\n",
    "origin_counts.plot(kind='barh', figsize=(8,4), title='Top Origin Airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. What airports have the most delayed departures and arrivals?\n",
    "\n",
    "A flight is delayed if it leaves or arrives more than 15 minutes after its scheduled time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter departure flights\n",
    "\n",
    "df_delayed_departure = df_csv[df_csv.depdelay > 15]\n",
    "delayed_counts = df_delayed_departure.origin.value_counts().head(10)\n",
    "print('Top delayed origin airports')\n",
    "print(delayed_counts)\n",
    "\n",
    "delayed_counts.plot(kind='barh', figsize=(8,4), title='Top Delayed Origin Airports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter arrival flights\n",
    "\n",
    "df_delayed_arrival = df_csv[df_csv.arrdelay > 15]\n",
    "delayed_counts = df_delayed_arrival.dest.value_counts().head(10)\n",
    "print('Top delayed destination airports')\n",
    "print(delayed_counts)\n",
    "\n",
    "delayed_counts.plot(kind='barh', figsize=(8,4), title='Top Delayed Destination Airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What flights are most frequently delayed with same origin and destination?\n",
    "\n",
    "A flight is delayed if it leaves or arrives more than 15 minutes after its scheduled time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to delayed flights\n",
    "\n",
    "df_filtered = df_csv.loc[(df_csv.arrdelay > 15) or (df_csv.depdelay > 15)].compute()\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group filtered dataset by origin, dest, and flightnum\n",
    "\n",
    "grp = df_filtered.groupby(['origin', 'dest', 'flightnum']) \\\n",
    ".flightnum.count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "\n",
    "grp.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "grp.head(15).set_index('flightnum').plot(kind='barh', figsize=(8,4), \\\n",
    "                                         title='Top Delayed Flights with Same Origin and Destination Airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Can you predict a flight's delayed minutes?\n",
    "\n",
    "Create a prediction model to predict flight delays. Dependent features like weather were not added because you don't know the weather accurately for the day. A new feature was added named `hdays` to indicate how many days the flight was from a holiday. Holidays have a significant impact on travel. Other key features might be helpful, but time did not permit further exploration.\n",
    "\n",
    "The work is borrowed from `https://jessesw.com/Air-Delays/`,  `https://gist.github.com/mrocklin/19c89d78e34437e061876a9872f4d2df`, and `https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to determine the difference between flight date and neares holiday.\n",
    "# see https://jessesw.com/Air-Delays/\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start='1987-01-01', end='2008-12-31').to_pydatetime()\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "def days_until_holiday(d):\n",
    "    ans = timedelta(100000)\n",
    "    for i in range(len(holidays)):\n",
    "        candidate = abs(holidays[i]-d)\n",
    "        if candidate < ans:\n",
    "            ans = candidate\n",
    "    return(float(ans.days))\n",
    "\n",
    "#assert(days_until_holiday(datetime(2001, 1, 1))==0)\n",
    "#assert(days_until_holiday(datetime(2009, 1, 1))==7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using apply is slow. Need to see if there is a faster way.\n",
    "\n",
    "# create days from nearest holiday column\n",
    "df_csv['hdays'] = df_csv.apply(lambda r: days_until_holiday(datetime(int(r.year), int(r.month), int(r.dayofmonth))),\n",
    "                               meta=float, axis=1)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop columns we don't need or want\n",
    "df_csv = df_csv.drop(['deptime', 'arrtime', 'flightnum'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample of data (features)\n",
    "df = df_csv.sample(frac=0.05)\n",
    "\n",
    "# drop rows with NaN\n",
    "prev_count = len(df)\n",
    "df = df.dropna()\n",
    "number_of_items = len(df)\n",
    "print('dropped %s percent of rows with NA' % locale.format('%.2f', prev_count/number_of_items))\n",
    "\n",
    "# target data\n",
    "y = df.depdelay\n",
    "\n",
    "df = df.drop(['depdelay'], axis=1)\n",
    "\n",
    "df, y = persist(df, y)\n",
    "progress(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_items = len(df)\n",
    "locale.format('%d', number_of_items, grouping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorize appropriate columns\n",
    "\n",
    "object_columns = ['uniquecarrier', 'origin', 'dest', 'year', 'month', 'dayofmonth', 'dayofweek']\n",
    "for i in object_columns:\n",
    "    df_csv[i] = df_csv[i].astype('category')\n",
    "\n",
    "df_csv = df_csv.categorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dd.get_dummies(df.categorize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test = X.random_split([0.9, 0.1], random_state=1234)\n",
    "labels_train, labels_test = y.random_split([0.9, 0.1], random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "y_true, y_pred = labels_test, model.predict(data_test)\n",
    "    \n",
    "print('Mean absolute error of regression was:')\n",
    "print(locale.format('%.1f', mean_absolute_error(y_true, y_pred), grouping=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
