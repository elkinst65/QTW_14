{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=toc></a>\n",
    "# MSDS 7333 - Week 14 Case Study: Analyzing Airline Flight Delays\n",
    "\n",
    "### Investigators\n",
    "- [Matt Baldree](mailto:mbaldree@smu.edu?subject=lab14)\n",
    "- [Ben Brock](bbrock@smu.edu?subject=lab14)\n",
    "- [Tom Elkins](telkins@smu.edu?subject=lab14)\n",
    "- [Austin Kelly](ajkelly@smu.edu?subject=lab14)\n",
    "\n",
    "\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:5px;'>\n",
    "    <h3>Instructions</h3>\n",
    "    <p>Work with the airline data set (use R or Python to manage out-of-core).</p>\n",
    "     <p>Answer the following questions by using the split-apply-combine technique</p>\n",
    "    <ol>\n",
    "        <li>Which airports are most likely to be delayed flying out of or into?</li>\n",
    "        <li>Which flights with same origin and destination are most likely to be delayed?</li>\n",
    "        <li>Can you regress how delayed a flight will be before it is delayed?</li>\n",
    "        <li>What are the most important features for this regression?\n",
    "            <ul>\n",
    "            <li>Remember to properly cross-validate models.\n",
    "            <li>Use meaningful evaluation criteria.\n",
    "            <li>Create at least one new feature variable for the regression.\n",
    "            </ul>\n",
    "            \n",
    "    </ol> \n",
    "            \n",
    "\n",
    "    <p>Report Sections:</p>\n",
    "    <ol>\n",
    "        <li>[Introduction](#introduction) <b>(5 points)</b></li>\n",
    "        <li>[Background](#background) <b>(10 points)</b></li>\n",
    "        <li>[Methods](#methods) <b>(30 points)</b></li>\n",
    "        <li>[Results](#results) <b>(30 points)</b></li>\n",
    "        <li>[Conclusion](#conclusion) <b>(5 points)</b></li>\n",
    "        <li>[Bibliography and Citation](#biblio) <b>(5 points)</b></li>\n",
    "        <li>[Code](#code) <b>(5 points)</b></li>\n",
    "    </ol>\n",
    "     <p>Other Grading Criterium:</p>\n",
    "    <ol>\n",
    "        <li>Grammar and Organization <b>(10 points)</b></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='introduction'></a>\n",
    "## 1 - Introduction\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:10px;'>\n",
    "<h3>Introduction (<b>5 points total</b>)</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain what  we will be working with and how we intend to approach the issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case study, we are tasked with acquiring and combining airline data from 22 separate years of airline history. Once the data is downloaded, it will be parsed and appended to a data frame in which we will be able to determine the statistics of said data. With such a large amount of data, it will be notably difficult to be able to use conventional methods to aggregate and perform calculations with conventional methods. \n",
    "\n",
    "The data in question totals just over 123.5 Million records and sizes up to be about 14 Gigabytes **uncompressed** of just csv data. \n",
    "\n",
    "That's a lot of data.\n",
    "\n",
    "In order to be able to not only handle the data but also perform calculations over the dataframe, we will need to utilize more than just a single core of the (current) 4-core processors embedded within our machines. When more than a single processor core is utilized, we venture into the realm of parallel computing. As we parse and sift through the data, parallel computig allows for a rather novel idea: break the data down into even parts and process all three parts at the same time. Many titans of industry use platforms such as Hadoop Distributed File System (HDFS) to manage massive amounts of data relatively quickly with clusters of commodity servers. When a massive datafile comes through (in our case, 12-14 Gb), instead of just using a single core to process all of the data, we will use three cores to process 4-5 Gb of data _each_, leaving a spare core (the master) to manage all three cores.\n",
    "\n",
    "For this case study, we were met with many roadblocks such as software compatibility with hardware along with version control. We found it was quite difficult to manage older versions of R alongside the newest version of Python, all in the same Jupyter notebook. To minimize these roadblocks, our team utilized the Python 3.4 package [Dask](https://dask.pydata.org/en/latest/) along with Python 2.7's [Graphlab-Create](https://turi.com/). Once these processes were executed in their entirety, we decided to cross-validate our findings by generating an equivalent Javascript environment to independently test our findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"background\"></a>\n",
    "## 2 - Background\n",
    "\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:10px;'>\n",
    "<h3>Background (<b>10 points total</b>)</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset our group acquired was comprised of just over 123 Million records with 29 attributes. These attributes are described in this table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable descriptions of original data set\n",
    "|Item|Name|Description|\n",
    "|:--:|:--|:--|\n",
    "|1|\tYear\t|1987-2008|\n",
    "|2|\tMonth\t|1-12|\n",
    "|3|\tDayofMonth\t|1-31|\n",
    "|4|\tDayOfWeek\t|1 (Monday) - 7 (Sunday)|\n",
    "|5|\tDepTime\t|actual departure time (local, hhmm)|\n",
    "|6|\tCRSDepTime\t|scheduled departure time (local, hhmm)|\n",
    "|7|\tArrTime\tactual |arrival time (local, hhmm)|\n",
    "|8|\tCRSArrTime\t|scheduled arrival time (local, hhmm)|\n",
    "|9|\tUniqueCarrier\t|unique carrier code|\n",
    "|10|\tFlightNum\t|flight number|\n",
    "|11|\tTailNum\tplane |tail number|\n",
    "|12|\tActualElapsedTime\t|in minutes|\n",
    "|13|\tCRSElapsedTime\t|in minutes|\n",
    "|14|\tAirTime\t|in minutes|\n",
    "|15|\tArrDelay\t|arrival delay, in minutes|\n",
    "|16|\tDepDelay\t|departure delay, in minutes|\n",
    "|17|\tOrigin\t|origin IATA airport code|\n",
    "|18|\tDest\t|destination IATA airport code|\n",
    "|19|\tDistance\t|in miles|\n",
    "|20|\tTaxiIn\t|taxi in time, in minutes|\n",
    "|21|\tTaxiOut\t|taxi out time in minutes|\n",
    "|22|\tCancelled\t|was the flight cancelled?|\n",
    "|23|\tCancellationCode\t|reason for cancellation (A = carrier, B = weather, C = NAS, D = security)|\n",
    "|24|\tDiverted\t|1 = yes, 0 = no|\n",
    "|25|\tCarrierDelay\t|in minutes|\n",
    "|26|\tWeatherDelay\t|in minutes|\n",
    "|27|\tNASDelay\t|in minutes|\n",
    "|28|\tSecurityDelay\t|in minutes|\n",
    "|29|\tLateAircraftDelay\t|in minutes|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three most-important (and required) questions are:\n",
    "\n",
    "(click on each question to navigate to the section of the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>[Q1.What airports have the most delayed departures and arrivals?](#Question1)</li> \n",
    "<li>[Q2. What flights are most frequently delayed with same origin and destination?](#Question2)</li>\n",
    "<li>[Q3. Can you predict a flight's delayed time in minutes?](#Question3)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these questions seem obvious to us, it is important to clearly identify our intent of what we are looking to explore in order to discover an appropriate answer to the proper questions. \n",
    "\n",
    "First and foremost, we will want to investigate (using the basic aggregation functions) just which airports are the main culprits for delayed departures and which are subject to the late arrivals. It must be declared a flight is considered to be delayed if it leaves or arrives more than 15 minutes from it's scheduled time. Something to be investigated at a later date (when adequate resources are available) is whether or not the late departures influence the late arrivals more than the late arrivals affect the late departures.\n",
    "\n",
    "The second question begs investigation into whether or not there is a specific route plagued with said delays. With so many unique routes, it will be interesting to see whether or not one route really sticks out over the rest. Since our analysis is limited to our data, we will not be seeing mnay \"entire\" routes. This is attributed to the simple fact that many entire routes (e.g. New York to Los Angeles) are typically comprised of multiple sub-routes. Thus, we will be focusing on the routes which comprise the longer routes. \n",
    "\n",
    "With all of the data we have at our disposal, we will explore the possibility of being able to predict just _how_ delayed a flight will be based on the many factors involved. While we do have numerous factors to possibly influence the outcome of our predictions, there are also several factors outside of the scope of this study that will be considered to be confounding variables. One such variable is the weather of the locations involved. As difficult as it may be to predict the delay of a particular flight based on the day of the week coupled with the carrier, it will be far more difficult to predict exact snowfall along with wind speed for the area in question, ultimately grounding unsuspecting travelers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"methods\"></a>\n",
    "## 3 - Methods\n",
    "\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:10px;'>\n",
    "<h3>Methods (<b>30 points total</b>)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disuss Dask approach and our decision to go this route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general stuff\n",
    "import locale\n",
    "# locale.setlocale(locale.LC_ALL, 'en_US')\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('compute.use_bottleneck', True)\n",
    "pd.set_option('compute.use_numexpr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask import compute, persist\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flags\n",
    "\n",
    "PURGE_DATA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='color:red'>\n",
    "<h3>&darr; consider turning the results of this into a screenshot of whatever machine we use to do our final pass. </h3></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tornado.application - ERROR - Exception in callback functools.partial(<function wrap.<locals>.null_wrapper at 0x0000022991DEC730>, <tornado.concurrent.Future object at 0x0000022991DE7518>)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\site-packages\\bokeh\\server\\tornado.py\", line 437, in _start_async\n",
      "    signal.signal(signal.SIGTERM, self._sigterm)\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\signal.py\", line 47, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "ValueError: signal only works in main thread\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 604, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 619, in <lambda>\n",
      "    self.add_future(ret, lambda f: f.result())\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\site-packages\\tornado\\concurrent.py\", line 237, in result\n",
      "    raise_exc_info(self._exc_info)\n",
      "  File \"<string>\", line 3, in raise_exc_info\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 270, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\types.py\", line 248, in wrapped\n",
      "    coro = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\austi\\Anaconda3\\lib\\site-packages\\bokeh\\server\\tornado.py\", line 439, in _start_async\n",
      "    self.exit(1)\n",
      "AttributeError: 'BokehTornado' object has no attribute 'exit'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:50358\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787' target='_blank'>http://127.0.0.1:8787</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>2.53 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:50358' processes=4 cores=4>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start Dask distributed client and print out stats\n",
    "\n",
    "c = Client()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if loading data, then purge existing data directory.\n",
    "\n",
    "if PURGE_DATA:\n",
    "    # delete /data directory\n",
    "    from shutil import rmtree\n",
    "\n",
    "    path = 'data'\n",
    "    if os.path.exists(path):\n",
    "        rmtree(path)\n",
    "    \n",
    "    # make /data if it doesn't exist\n",
    "    path = 'data'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 3s\n",
      "downloaded file: data/1987.csv , of size (MB): 127.2\n",
      "downloaded file: data/1988.csv , of size (MB): 501.0\n",
      "downloaded file: data/1989.csv , of size (MB): 486.5\n",
      "downloaded file: data/1990.csv , of size (MB): 509.2\n",
      "downloaded file: data/1991.csv , of size (MB): 491.2\n",
      "downloaded file: data/1992.csv , of size (MB): 492.3\n",
      "downloaded file: data/1993.csv , of size (MB): 490.8\n",
      "downloaded file: data/1994.csv , of size (MB): 501.6\n",
      "downloaded file: data/1995.csv , of size (MB): 530.8\n",
      "downloaded file: data/1996.csv , of size (MB): 533.9\n",
      "downloaded file: data/1997.csv , of size (MB): 540.3\n",
      "downloaded file: data/1998.csv , of size (MB): 538.4\n",
      "downloaded file: data/1999.csv , of size (MB): 552.9\n",
      "downloaded file: data/2000.csv , of size (MB): 570.2\n",
      "downloaded file: data/2001.csv , of size (MB): 600.4\n",
      "downloaded file: data/2002.csv , of size (MB): 530.5\n",
      "downloaded file: data/2003.csv , of size (MB): 626.7\n",
      "downloaded file: data/2004.csv , of size (MB): 669.9\n",
      "downloaded file: data/2005.csv , of size (MB): 671.0\n",
      "downloaded file: data/2006.csv , of size (MB): 672.1\n",
      "downloaded file: data/2007.csv , of size (MB): 702.9\n",
      "downloaded file: data/2008.csv , of size (MB): 689.4\n",
      "Number of files downloaded: 22 for a total size (MB): 12029.2\n"
     ]
    }
   ],
   "source": [
    "# if loading data, download files and decompress them in parallel.\n",
    "\n",
    "DOWNLOAD_ONE_FILE_ONLY = False\n",
    "\n",
    "if PURGE_DATA:\n",
    "    import urllib.request\n",
    "    import shutil\n",
    "    import bz2\n",
    "    \n",
    "    def download_file(baseurl, yr):\n",
    "        file_name = ''\n",
    "\n",
    "        url_of_data_file = baseurl%(yr)\n",
    "        file_name = 'data/%d.csv'%(yr)\n",
    "        size = 0\n",
    "\n",
    "        print('downloading', url_of_data_file, 'to', file_name)\n",
    "        decompressor = bz2.BZ2Decompressor()\n",
    "\n",
    "        # download file and decompress it\n",
    "        with urllib.request.urlopen(url_of_data_file) as response, open(file_name, 'wb') as out_file:\n",
    "                data = decompressor.decompress(response.read())\n",
    "                out_file.write(data)\n",
    "                size = len(data)\n",
    "                print('file size (MB)', locale.format('%.1f', size/1000000, grouping=True))\n",
    "\n",
    "        return(file_name, size)\n",
    "    \n",
    "    def print_files(files):\n",
    "        totalSize = 0        \n",
    "        for f in files:\n",
    "            size = f[1]/1000000\n",
    "            totalSize += size\n",
    "            print('downloaded file:', f[0], ', of size (MB):', \n",
    "                  locale.format('%.1f', size, grouping=True))\n",
    "            \n",
    "        print('Number of files downloaded:', len(files), 'for a total size (MB):', \n",
    "              locale.format('%.1f', totalSize, grouping=True))\n",
    "\n",
    "\n",
    "    if DOWNLOAD_ONE_FILE_ONLY:\n",
    "        # testing\n",
    "        download_file('http://stat-computing.org/dataexpo/2009/%d.csv.bz2', 1988)\n",
    "    else:    \n",
    "        # download airline data from 1987 to 2009\n",
    "        yrs = range(1987, 2009)\n",
    "        baseurl = 'http://stat-computing.org/dataexpo/2009/%d.csv.bz2'\n",
    "\n",
    "        from dask import delayed\n",
    "        download_file = delayed(download_file)\n",
    "\n",
    "        files = [download_file(baseurl, yr) for yr in yrs]\n",
    "        files = delayed(files)\n",
    "\n",
    "        %time files = files.compute()   \n",
    "      \n",
    "        print_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv file format\n",
      "Loading data\\*.csv files\n",
      "Wall time: 523 ms\n",
      "year                 float64\n",
      "month                float64\n",
      "dayofmonth           float64\n",
      "dayofweek            float64\n",
      "deptime              float64\n",
      "crsdeptime           float64\n",
      "arrtime              float64\n",
      "crsarrtime           float64\n",
      "uniquecarrier         object\n",
      "flightnum            float64\n",
      "tailnum               object\n",
      "actualelapsedtime    float64\n",
      "crselapsedtime       float64\n",
      "airtime              float64\n",
      "arrdelay             float64\n",
      "depdelay             float64\n",
      "origin                object\n",
      "dest                  object\n",
      "distance             float64\n",
      "taxiin               float64\n",
      "taxiout              float64\n",
      "cancelled            float64\n",
      "cancellationcode      object\n",
      "diverted             float64\n",
      "carrierdelay         float64\n",
      "weatherdelay         float64\n",
      "nasdelay             float64\n",
      "securitydelay        float64\n",
      "lateaircraftdelay    float64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>deptime</th>\n",
       "      <th>crsdeptime</th>\n",
       "      <th>arrtime</th>\n",
       "      <th>crsarrtime</th>\n",
       "      <th>uniquecarrier</th>\n",
       "      <th>flightnum</th>\n",
       "      <th>tailnum</th>\n",
       "      <th>actualelapsedtime</th>\n",
       "      <th>crselapsedtime</th>\n",
       "      <th>airtime</th>\n",
       "      <th>arrdelay</th>\n",
       "      <th>depdelay</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>distance</th>\n",
       "      <th>taxiin</th>\n",
       "      <th>taxiout</th>\n",
       "      <th>cancelled</th>\n",
       "      <th>cancellationcode</th>\n",
       "      <th>diverted</th>\n",
       "      <th>carrierdelay</th>\n",
       "      <th>weatherdelay</th>\n",
       "      <th>nasdelay</th>\n",
       "      <th>securitydelay</th>\n",
       "      <th>lateaircraftdelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>912.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1451.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>SAN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>447.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>903.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1451.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>SAN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>447.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1451.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>SAN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>447.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>847.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1451.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>SAN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>447.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>749.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>922.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>PS</td>\n",
       "      <td>1451.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>SAN</td>\n",
       "      <td>SFO</td>\n",
       "      <td>447.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  month  dayofmonth  dayofweek  deptime  crsdeptime  arrtime  \\\n",
       "0  1987.0   10.0        14.0        3.0    741.0       730.0    912.0   \n",
       "1  1987.0   10.0        15.0        4.0    729.0       730.0    903.0   \n",
       "2  1987.0   10.0        17.0        6.0    741.0       730.0    918.0   \n",
       "3  1987.0   10.0        18.0        7.0    729.0       730.0    847.0   \n",
       "4  1987.0   10.0        19.0        1.0    749.0       730.0    922.0   \n",
       "\n",
       "   crsarrtime uniquecarrier  flightnum tailnum  actualelapsedtime  \\\n",
       "0       849.0            PS     1451.0     NaN               91.0   \n",
       "1       849.0            PS     1451.0     NaN               94.0   \n",
       "2       849.0            PS     1451.0     NaN               97.0   \n",
       "3       849.0            PS     1451.0     NaN               78.0   \n",
       "4       849.0            PS     1451.0     NaN               93.0   \n",
       "\n",
       "   crselapsedtime  airtime  arrdelay  depdelay origin dest  distance  taxiin  \\\n",
       "0            79.0      NaN      23.0      11.0    SAN  SFO     447.0     NaN   \n",
       "1            79.0      NaN      14.0      -1.0    SAN  SFO     447.0     NaN   \n",
       "2            79.0      NaN      29.0      11.0    SAN  SFO     447.0     NaN   \n",
       "3            79.0      NaN      -2.0      -1.0    SAN  SFO     447.0     NaN   \n",
       "4            79.0      NaN      33.0      19.0    SAN  SFO     447.0     NaN   \n",
       "\n",
       "   taxiout  cancelled cancellationcode  diverted  carrierdelay  weatherdelay  \\\n",
       "0      NaN        0.0              NaN       0.0           NaN           NaN   \n",
       "1      NaN        0.0              NaN       0.0           NaN           NaN   \n",
       "2      NaN        0.0              NaN       0.0           NaN           NaN   \n",
       "3      NaN        0.0              NaN       0.0           NaN           NaN   \n",
       "4      NaN        0.0              NaN       0.0           NaN           NaN   \n",
       "\n",
       "   nasdelay  securitydelay  lateaircraftdelay  \n",
       "0       NaN            NaN                NaN  \n",
       "1       NaN            NaN                NaN  \n",
       "2       NaN            NaN                NaN  \n",
       "3       NaN            NaN                NaN  \n",
       "4       NaN            NaN                NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the head of a csv file\n",
    "\n",
    "print('csv file format')\n",
    "# !head data/1987.csv\n",
    "\n",
    "# load csv files into a dataframe in parallel\n",
    "\n",
    "filename = os.path.join('data', '*.csv')\n",
    "print('Loading', filename, 'files')\n",
    "\n",
    "import dask.dataframe as dd\n",
    "%time df_csv = dd.read_csv(filename, assume_missing=True, \\\n",
    "                           dtype={'TailNum':np.object, 'CancellationCode':np.object}, \\\n",
    "                           storage_options={'anon': True}).rename(columns=str.lower)\n",
    "\n",
    "print(df_csv.dtypes)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop columns we don't need or want\n",
    "df_csv = df_csv.drop(['tailnum', 'actualelapsedtime', 'crselapsedtime', 'airtime', \\\n",
    "             'taxiin', 'taxiout', 'cancelled', 'cancellationcode', \\\n",
    "             'diverted', 'carrierdelay', 'weatherdelay', 'nasdelay', \\\n",
    "             'securitydelay', 'lateaircraftdelay'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# categorize appropriate columns\n",
    "\n",
    "object_columns = ['uniquecarrier', 'origin', 'dest', 'year', 'month', 'dayofmonth', 'dayofweek']\n",
    "for i in object_columns:\n",
    "    df_csv[i] = df_csv[i].astype('category')\n",
    "\n",
    "df_csv = df_csv.categorize()\n",
    "df_csv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-166f82814833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# length of dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnumber_of_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrouping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_csv' is not defined"
     ]
    }
   ],
   "source": [
    "# length of dataframe\n",
    "\n",
    "number_of_items = len(df_csv)\n",
    "locale.format('%d', number_of_items, grouping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 10 origin airports\n",
    "\n",
    "origin_counts = df_csv.origin.value_counts().head(10)\n",
    "print('Top origin airports')\n",
    "print(origin_counts)\n",
    "\n",
    "origin_counts.plot(kind='barh', figsize=(8,4), title='Top Origin Airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=Question1></a>\n",
    "## Q1. What airports have the most delayed departures and arrivals?\n",
    "\n",
    "A flight is delayed if it leaves or arrives more than 15 minutes after its scheduled time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter departure flights\n",
    "\n",
    "df_delayed_departure = df_csv[df_csv.depdelay > 15]\n",
    "delayed_counts = df_delayed_departure.origin.value_counts().head(10)\n",
    "print('Top delayed origin airports')\n",
    "print(delayed_counts)\n",
    "\n",
    "delayed_counts.plot(kind='barh', figsize=(8,4), title='Top Delayed Origin Airports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter arrival flights\n",
    "\n",
    "df_delayed_arrival = df_csv[df_csv.arrdelay > 15]\n",
    "delayed_counts = df_delayed_arrival.dest.value_counts().head(10)\n",
    "print('Top delayed destination airports')\n",
    "print(delayed_counts)\n",
    "\n",
    "delayed_counts.plot(kind='barh', figsize=(8,4), title='Top Delayed Destination Airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = Question2></a>\n",
    "## Q2. What flights are most frequently delayed with same origin and destination?\n",
    "\n",
    "A flight is delayed if it leaves or arrives more than 15 minutes after its scheduled time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter the dataset to delayed flights\n",
    "\n",
    "df_filtered = df_csv.loc[(df_csv.arrdelay > 15) or (df_csv.depdelay > 15)].compute()\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorize appropriate columns\n",
    "\n",
    "object_columns = ['origin', 'dest', 'flightnum']\n",
    "for i in object_columns:\n",
    "    df_filtered[i] = df_filtered[i].astype('category')\n",
    "\n",
    "df_filtered = df_filtered.categorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_filtered.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Group filtered dataset by origin, dest, and flightnum\n",
    "\n",
    "grp = df_filtered.groupby(['origin', 'dest', 'flightnum']) \\\n",
    ".flightnum.count().reset_index(name='count').sort_values(['count'], ascending=False)\n",
    "\n",
    "grp.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "grp.head(15).plot(kind='barh', figsize=(8,4), \\\n",
    "                  title='Top Delayed Flights with Same Origin and Destination Airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=Question3></a>\n",
    "## Q3. Can you predict a flight's delayed time in minutes?\n",
    "\n",
    "Create a prediction model to predict flight delays. Dependent features like weather were not added because you don't know the weather accurately for the day. A new feature was added named `hdays` to indicate how many days the flight was from a holiday. Holidays have a significant impact on travel. Other key features might be helpful, but time did not permit further exploration.\n",
    "\n",
    "[&darr; we should probably add this to the end &darr;](#biblio)\n",
    "\n",
    "The work is borrowed from \n",
    "> `https://jessesw.com/Air-Delays/`,  \n",
    "> `https://gist.github.com/mrocklin/19c89d78e34437e061876a9872f4d2df`, and \n",
    "> `https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to determine the difference between flight date and nearest holiday.\n",
    "# see https://jessesw.com/Air-Delays/\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays(start='1987-01-01', end='2008-12-31').to_pydatetime()\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "def days_until_holiday(d):\n",
    "    ans = timedelta(100000)\n",
    "    for i in range(len(holidays)):\n",
    "        candidate = abs(holidays[i]-d)\n",
    "        if candidate < ans:\n",
    "            ans = candidate\n",
    "    return(float(ans.days))\n",
    "\n",
    "#assert(days_until_holiday(datetime(2001, 1, 1))==0)\n",
    "#assert(days_until_holiday(datetime(2009, 1, 1))==7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random sample of data (features)\n",
    "df = df_csv.sample(frac=0.05)\n",
    "\n",
    "# drop rows with NaN\n",
    "prev_count = len(df)\n",
    "df = df.dropna()\n",
    "number_of_items = len(df)\n",
    "print('dropped %s percent of rows with NA' % locale.format('%.2f', prev_count/number_of_items))\n",
    "\n",
    "# target data\n",
    "y = df.depdelay\n",
    "\n",
    "df = df.drop(['depdelay'], axis=1)\n",
    "\n",
    "df, y = persist(df, y)\n",
    "progress(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using apply is slow. Need to see if there is a faster way.\n",
    "\n",
    "# create days from nearest holiday column\n",
    "df_csv['hdays'] = df_csv.apply(lambda r: days_until_holiday(datetime(int(r.year), int(r.month), int(r.dayofmonth))),\n",
    "                               meta=float, axis=1)\n",
    "\n",
    "# convert scheduled time into hrs\n",
    "df_csv['depthr'] = df_csv.apply(lambda r: np.trunc(r.crsdeptime / 100.), meta='category', axis=1)\n",
    "df_csv['arrhr'] = df_csv.apply(lambda r: np.trunc(r.crsarrtime / 100.), meta='category', axis=1)\n",
    "df_csv.depthr = df_csv.depthr.astype('category')\n",
    "df_csv.arrhr = df_csv.arrhr.astype('category')\n",
    "\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop columns we don't need or want\n",
    "df_csv = df_csv.drop(['deptime', 'arrtime', 'flightnum', 'crsarrtime', 'crsdeptime'], axis=1)\n",
    "df_csv.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorize appropriate columns\n",
    "\n",
    "object_columns = ['uniquecarrier', 'origin', 'dest', 'year', 'month', 'dayofmonth', 'dayofweek']\n",
    "for i in object_columns:\n",
    "    df[i] = df[i].astype('category')\n",
    "\n",
    "df = df.categorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random sample of data (features)\n",
    "df = df_csv.sample(frac=0.2)\n",
    "\n",
    "# drop rows with NaN\n",
    "prev_count = len(df)\n",
    "df = df.dropna()\n",
    "number_of_items = len(df)\n",
    "print('dropped %s percent of rows with NA' % locale.format('%.2f', prev_count/number_of_items))\n",
    "\n",
    "# target data\n",
    "y = df.depdelay\n",
    "\n",
    "df = df.drop(['depdelay'], axis=1)\n",
    "\n",
    "df, y = persist(df, y)\n",
    "progress(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dd.get_dummies(df.categorize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test = X.random_split([0.9, 0.1], random_state=1234)\n",
    "labels_train, labels_test = y.random_split([0.9, 0.1], random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(data_train)\n",
    "# data_train = scaler.transform(data_train)\n",
    "# data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "model = BayesianRidge()\n",
    "model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell was interrupted by keyboard. Likely where we're going to leave it for the night.\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit = delayed(model.fit)\n",
    "model.fit(data_train, labels_train).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model = SGDRegressor(random_state = 0) \n",
    "model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit = delayed(model.fit)\n",
    "model.fit(data_train, labels_train).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "y_true, y_pred = labels_test, model.predict(data_test)\n",
    "    \n",
    "print('Mean absolute error of SGD regression was:')\n",
    "print(locale.format('%.1f', mean_absolute_error(y_true, y_pred), grouping=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss graphlab create approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss javascript approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "## 4 - Results\n",
    "\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:10px;'>\n",
    "<h3>Results (<b>30 points total</b>)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss results of Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss results of gl-create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss results of javascript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Talk about how they all tie in (cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## 5 - Conclusion\n",
    "\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:10px;'>\n",
    "<h3>Conclusion (<b>5 points total</b>)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what did we find and how did we answer all of the questions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"biblio\"></a>\n",
    "## 6 - Bibliography and Citation\n",
    "\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:10px;'>\n",
    "<h3>Bibliography and Citation (<b>5 points total</b>)</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"code\"></a>\n",
    "## 7 - Code\n",
    "\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:10px;'>\n",
    "<h3>Code (5 points)</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[&uarr; ToC](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
